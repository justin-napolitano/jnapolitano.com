<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Justin Napolitano</title>
    <link>//localhost:1313/en/series/java/</link>
    <description>Recent content in Java on Justin Napolitano</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>personal.jnapolitano@gmail.com (Justin Napolitano)</managingEditor>
    <webMaster>personal.jnapolitano@gmail.com (Justin Napolitano)</webMaster>
    <copyright>COBRACORP</copyright>
    <lastBuildDate>Thu, 08 Aug 2024 14:11:15 -0500</lastBuildDate>
    <atom:link href="//localhost:1313/en/series/java/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Neo4J: Supreme Court Explore</title>
      <link>//localhost:1313/en/posts/sup-court-graph-explore/</link>
      <pubDate>Thu, 08 Aug 2024 14:10:52 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/sup-court-graph-explore/</guid>
      <description>Supreme Court Graph Explore In the past few posts, I built a knowledge graph based on some metadata. It was an enjoyable experience, but what is the real purpose behind it?&#xA;Summary of Work So Far Build the Database first I a proponent of building out an immutable source of truth prior to ingesting anything into a graph. I have in the past built graphs for fun, but found that transforming in python then loading into memory prior to ingesting into neo was a painful and expensive exercise.</description>
    </item>
    <item>
      <title>PostgreSQL to Neo4j Transfer: Unique Nodes and DB Optimization</title>
      <link>//localhost:1313/en/posts/supreme-court-transfer-service-uniques/</link>
      <pubDate>Wed, 07 Aug 2024 14:59:58 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/supreme-court-transfer-service-uniques/</guid>
      <description>Optimizing Graph Databases: Ensuring Unique Nodes and Creating Efficient Relationships in Neo4j repo source : https://github.com/justin-napolitano/supreme-court-transfer&#xA;In my last post I kinda messed up.. well not really but I did not account for trying to create unique relationships.. That doesn&amp;rsquo;t really matter though because my postgres db is normalized.. so i can make those relationships easy. This post details how to do that.&#xA;Importance of Normalization When working with large datasets, particularly in a relational database, ensuring the uniqueness of nodes and creating efficient relationships in a graph database like Neo4j can be challenging.</description>
    </item>
    <item>
      <title>Postgres to Neo4j Workflow: Relationships</title>
      <link>//localhost:1313/en/posts/supreme-court-meta-relationships/</link>
      <pubDate>Tue, 06 Aug 2024 16:10:02 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/supreme-court-meta-relationships/</guid>
      <description>Updating the PostGres to Neo workflow with Standard Meta Data Relationships In my previous post i detailed how to export data from postgresql to neo4j with a java workflow.. but I did not add how add relationships once the node insert completes.&#xA;Here is the repo btw https://github.com/justin-napolitano/supreme-court-transfer&#xA;RelationshipCreator.java package com.supreme_court_transfer; import org.neo4j.driver.Session; import org.neo4j.driver.Transaction; import org.neo4j.driver.TransactionWork; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class RelationshipCreator { private static final Logger logger = LoggerFactory.</description>
    </item>
    <item>
      <title>Postgres to Neo4j Workflow</title>
      <link>//localhost:1313/en/posts/supreme-court-to-postgres-to-neo/</link>
      <pubDate>Tue, 06 Aug 2024 15:10:02 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/supreme-court-to-postgres-to-neo/</guid>
      <description>Streamlining Data Transfer from PostgreSQL to Neo4j with Java In my previous few posts I put together a postgres db of supreme court meta data. That is cool. I have a few more ideas.. but I wanted to explore the graph a bit before expanding it with some nlp. So in this post I am writing about how to ingest a bunch of nodes into neo4j with a java workflow. In the next post I&amp;rsquo;ll detail adding relationships.</description>
    </item>
    <item>
      <title>PERN Stack with GraphQl and Apollo</title>
      <link>//localhost:1313/en/posts/sup-court-pern-stack/</link>
      <pubDate>Fri, 02 Aug 2024 15:16:27 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/sup-court-pern-stack/</guid>
      <description>Setting Up a PERN Stack with GraphQL and Apollo: Source Repo : https://github.com/justin-napolitano/sup-court-pern-stack.git&#xA;I&amp;rsquo;ve been playing with a knowledge graph recently. Most of the data modelling is complete&amp;hellip; well at least the groundwork is done.&#xA;In this part of the series,we&amp;rsquo;ll walk through the process of setting up a PostgreSQL, Express, React, Node.js (PERN) stack application with GraphQL and Apollo.&#xA;The point is to build out a backend so I can then build out a react client to explore the beautiful grapth.</description>
    </item>
    <item>
      <title>PostGreSQL Java: Data Ingestion</title>
      <link>//localhost:1313/en/posts/sup-court-data-ingestion/</link>
      <pubDate>Wed, 31 Jul 2024 14:25:13 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>//localhost:1313/en/posts/sup-court-data-ingestion/</guid>
      <description>Setting Up a Data Ingestion Workflow with Java and Google Cloud Storage Introduction In this blog post, we will walk through setting up a data ingestion workflow using Java. The workflow will download JSON data from a Google Cloud Storage (GCS) bucket, parse it, and insert it into a PostgreSQL database. We will also handle unique constraint violations gracefully.&#xA;Prerequisites Java 11 or higher installed Maven installed PostgreSQL running locally (preferably in a Docker container) Google Cloud Storage bucket with JSON files Service account key for Google Cloud Storage Setting Up the Project 1.</description>
    </item>
  </channel>
</rss>
