<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Justin Napolitano</title>
    <link>jnapolitano.com/en/tags/java/</link>
    <description>Recent content in Java on Justin Napolitano</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>personal.jnapolitano@gmail.com (Justin Napolitano)</managingEditor>
    <webMaster>personal.jnapolitano@gmail.com (Justin Napolitano)</webMaster>
    <copyright>COBRACORP</copyright>
    <lastBuildDate>Wed, 07 Aug 2024 15:00:07 -0500</lastBuildDate>
    <atom:link href="jnapolitano.com/en/tags/java/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PostgreSQL to Neo4j Transfer: Unique Nodes and DB Optimization</title>
      <link>jnapolitano.com/en/posts/supreme-court-transfer-service-uniques/</link>
      <pubDate>Wed, 07 Aug 2024 14:59:58 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/supreme-court-transfer-service-uniques/</guid>
      <description>Optimizing Graph Databases: Ensuring Unique Nodes and Creating Efficient Relationships in Neo4j repo source : https://github.com/justin-napolitano/supreme-court-transfer&#xA;In my last post I kinda messed up.. well not really but I did not account for trying to create unique relationships.. That doesn&amp;rsquo;t really matter though because my postgres db is normalized.. so i can make those relationships easy. This post details how to do that.&#xA;Importance of Normalization When working with large datasets, particularly in a relational database, ensuring the uniqueness of nodes and creating efficient relationships in a graph database like Neo4j can be challenging.</description>
    </item>
    <item>
      <title>PostGreSQL Java: Data Ingestion</title>
      <link>jnapolitano.com/en/posts/sup-court-data-ingestion/</link>
      <pubDate>Wed, 31 Jul 2024 14:25:13 -0500</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/sup-court-data-ingestion/</guid>
      <description>Setting Up a Data Ingestion Workflow with Java and Google Cloud Storage Introduction In this blog post, we will walk through setting up a data ingestion workflow using Java. The workflow will download JSON data from a Google Cloud Storage (GCS) bucket, parse it, and insert it into a PostgreSQL database. We will also handle unique constraint violations gracefully.&#xA;Prerequisites Java 11 or higher installed Maven installed PostgreSQL running locally (preferably in a Docker container) Google Cloud Storage bucket with JSON files Service account key for Google Cloud Storage Setting Up the Project 1.</description>
    </item>
  </channel>
</rss>
