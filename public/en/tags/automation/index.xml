<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Automation on Justin Napolitano</title>
    <link>jnapolitano.com/en/tags/automation/</link>
    <description>Recent content in Automation on Justin Napolitano</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <managingEditor>personal.jnapolitano@gmail.com (Justin Napolitano)</managingEditor>
    <webMaster>personal.jnapolitano@gmail.com (Justin Napolitano)</webMaster>
    <copyright>COBRACORP</copyright>
    <lastBuildDate>Mon, 15 Jul 2024 13:14:28 -0500</lastBuildDate>
    <atom:link href="jnapolitano.com/en/tags/automation/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Using GitHub Template Repositories to Automate Script Deployment</title>
      <link>jnapolitano.com/en/posts/create_a_github_template_repo/</link>
      <pubDate>Thu, 27 Jun 2024 12:00:00 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/create_a_github_template_repo/</guid>
      <description>Using GitHub Template Repositories to Automate Script Deployment Managing multiple repositories can be a challenge, especially when you need to ensure that each one includes certain common scripts or configurations. GitHub&amp;rsquo;s template repositories feature can help streamline this process. In this post, we&amp;rsquo;ll walk through how to use a template repository to automatically include a gh_submodule_sync.sh script in every new repository you create.&#xA;Prerequisites GitHub CLI: Ensure you have the GitHub CLI installed.</description>
    </item>
    <item>
      <title>Hugo Build and Deploy GH Workflow</title>
      <link>jnapolitano.com/en/posts/gh-pages-workflow/</link>
      <pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/gh-pages-workflow/</guid>
      <description>Creating a GH Workflow to Build and Deploy a hugo site to gh-pages Why To simplify the build process.&#xA;Creating the Workflow create your yaml config file touch hugo.yaml Set the trigger and the environment defaults The code below creates a trigger on push from the main and the gh-pages branches. It also sets read and write permissions to permit executing code and building hugo.&#xA;on: # Runs on pushes targeting the default branch push: branches: - main # - pit # - ghpages - gh-pages # Allows you to run this workflow manually from the Actions tab workflow_dispatch: # Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages permissions: contents: read pages: write id-token: write # Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI Part 8: Case Nodes Sample Data</title>
      <link>jnapolitano.com/en/posts/legal-research-part-8/</link>
      <pubDate>Mon, 23 May 2022 16:30:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-8/</guid>
      <description>Introduction The legal Research with AI Series is expanding quickly. This is the 9th post related to it in someway. Building this pipeline and integrating multiple datasets into nodes is proving to be verbose.&#xA;This post documents merging Oyez and Library of Congress of data structured json files that represent nodes and hierarchal relationships.&#xA;Main Function The plan for this program is to:&#xA;Read the prepared dataframe created in the legal research part 7 post for each row of the df load a dictionary from the the libary of congress path and the oyez path Set keys on the Oyez dataset Write the updated Oyez dataset to file Review the work below:</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 5</title>
      <link>jnapolitano.com/en/posts/legal-research-part-5/</link>
      <pubDate>Sat, 21 May 2022 14:30:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-5/</guid>
      <description>Legal Research with AI: Part 5&amp;quot; In the previous posts in this series, I have downloaded the data required to build the neo4j graph. In this post, I will arrange the data into a data structure that will permit me to easily create graph nodes and most importantly relationships.&#xA;The Runner Program The raw structure of the data is organized by the results of the api requests. There are thus 80 cases per file.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 4</title>
      <link>jnapolitano.com/en/posts/legal-research-part-4/</link>
      <pubDate>Thu, 19 May 2022 22:30:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-4/</guid>
      <description>Conduct Legal Research with AI Part 4 This is the fourth post in a series documenting the process of building an ml pipeline used to train models to predict the outcomes of Supreme Court cases.&#xA;You can find the others at:&#xA;Part 1: blog.jnapolitano.io/neo4j_integration/ Part 2: blog.jnapolitano.io/constitution_to_neo/ Part 3: blog.jnapolitano.io/ai-proof-of-concept/ Modeling the Supreme Court Thankfully, much of the ground work has been done by contributors to The Washington University of St.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 3</title>
      <link>jnapolitano.com/en/posts/legal-research-part-3/</link>
      <pubDate>Wed, 18 May 2022 14:40:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-3/</guid>
      <description>Conduct Legal Research with AI: Part 3 This is the third post in a series documenting the process of building an ml pipeline that will be used to train models to predict the outcomes of Supreme Court Cases.&#xA;You can find the others at:&#xA;blog.jnapolitano.io/neo4j_integration/ blog.jnapolitano.io/constitution_to_neo/ Introduction In this post, I will be testing a sample TensorFlow pipeline against the Supreme Court Database maintained by the Washington University Law School in order to build a proof of concept model for a Supreme Court Graph Analysis project.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 2</title>
      <link>jnapolitano.com/en/posts/legal-reserch-part-2/</link>
      <pubDate>Tue, 17 May 2022 18:40:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-reserch-part-2/</guid>
      <description>Integrating the Constitution to Neo4j I am currenlty building a graph database of Supreme Court cases in neo4j to model the behavior and decison making of the court.&#xA;In this post, I include the classes that I will be using to create individual nodes for the articles, sections, clauses, and subclauses of the Consititution.&#xA;Later, these will be related to cases and subjecst in order to train a tensorflow algorithm to recommend case law by issue area and to predict the outcome of cases.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 1</title>
      <link>jnapolitano.com/en/posts/legal-research-part-1/</link>
      <pubDate>Mon, 16 May 2022 14:40:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-1/</guid>
      <description>Introduction In a previous post, I detailed the process of crawling the Library of Congress API to generate json files that could be intergrated into you DB of choice.&#xA;In this discussion, we will integrate JSON data into a Neo4j graph database.&#xA;Overview The process is fairly straightforward. The most difficult part is wrangling your json data into the right format for integration.&#xA;The main function first instantiates the database config informormation.</description>
    </item>
    <item>
      <title>Conduct Legal Research with AI: Part 0</title>
      <link>jnapolitano.com/en/posts/legal-research-part-0/</link>
      <pubDate>Mon, 16 May 2022 13:40:32 +0000</pubDate><author>personal.jnapolitano@gmail.com (Justin Napolitano)</author>
      <guid>jnapolitano.com/en/posts/legal-research-part-0/</guid>
      <description>Crawling the Library of Congress API Introduction The United States Library of Congress maintains a rest api for developers to crawl their collections. It is an open source tool that anyone can access in order to conduct research. Check out the documenation at https://libraryofcongress.github.io/data-exploration/.&#xA;Creating a crawler I took the approach of writing a generator that produces a search result page object that can be operated upon with each iteration.</description>
    </item>
  </channel>
</rss>
