<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bash on Justin Napolitano</title>
    <link>jnapolitano.com/tags/bash/</link>
    <description>Recent content in bash on Justin Napolitano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>jayburdindustries</copyright>
    <lastBuildDate>Sat, 13 Jul 2024 14:30:55 -0500</lastBuildDate><atom:link href="jnapolitano.com/tags/bash/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pull.. Commit.. And Push Bash Script</title>
      <link>jnapolitano.com/posts/push-commits/</link>
      <pubDate>Sat, 13 Jul 2024 14:30:55 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/push-commits/</guid>
      <description>Push Committed and Uncommitted Changes Script This script traverses a specified directory of git repositories, pulls updates for all branches, checks for committed and uncommitted changes across all branches, and pushes those changes to the remote repository. For uncommitted changes, it creates a new branch called uncommitted, commits the changes with a message detailing the original branch, and then pushes the new branch to the remote. For the main branch, if there are committed changes, it moves those changes to a new branch called bad-practice and then pushes that branch to the remote repository.</description>
    </item>
    
    <item>
      <title>Update All Repos Bash Script</title>
      <link>jnapolitano.com/posts/pull-all-repos/</link>
      <pubDate>Sat, 13 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/pull-all-repos/</guid>
      <description>Update Repositories Script This script recursively finds all git repositories in a specified directory and pulls the latest changes for each one.
Prerequisites  Bash shell Git installed Proper permissions to access and modify the repositories  Installation   Create the script: Save the following script to a file named update_repos.sh:
#!/bin/bash  # Define the default root directory where your repos are located DEFAULT_ROOT_DIR=&amp;#34;/home/cobra/Repos&amp;#34; # Use the provided argument as the root directory, or the default if none is provided ROOT_DIR=${1:-$DEFAULT_ROOT_DIR} echo &amp;#34;Starting update process for repositories in $ROOT_DIR&amp;#34; # Function to pull changes in a git repository pull_repo() { local repo_dir=$1 echo &amp;#34;Pulling updates in $repo_dir&amp;#34; cd &amp;#34;$repo_dir&amp;#34; || return git pull echo &amp;#34;Completed update in $repo_dir&amp;#34; cd - || return } # Export the function so it can be used by find -exec export -f pull_repo # Find all .</description>
    </item>
    
    <item>
      <title>Create and Deploy Cloud Run Job Script</title>
      <link>jnapolitano.com/posts/create_deploy_cloud_run_job/</link>
      <pubDate>Thu, 11 Jul 2024 16:26:32 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/create_deploy_cloud_run_job/</guid>
      <description>Cloud Run Job Deployment Script This repository contains a script to build and deploy a Python application as a Cloud Run Job using Google Cloud Build. The script dynamically generates a cloudbuild.yaml file and submits it to Google Cloud Build.
Prerequisites Before using the deployment script, ensure you have the following:
 Google Cloud SDK: Installed and configured. Docker: Installed. Google Cloud Project: Created and configured. Service Account Key: A service account key JSON file with appropriate permissions stored at keys/service-account-key.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run: LOC Flattener</title>
      <link>jnapolitano.com/posts/loc_normalizer/</link>
      <pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/loc_normalizer/</guid>
      <description>Library of Congress Normalizer Job This repo normalizes the existing library of congress schema into a db that wil then be used to construct a knowledge graph of supreme court law.
Plan  Setup a venv to run locally Install requirements Write out the script to interface with gcp Set up a docker container and test locally build the image upload to gcp create the job  Setup the venv Install I installed virtualenv locally on ubuntu</description>
    </item>
    
    <item>
      <title>Install MySQL Server on Ubuntu</title>
      <link>jnapolitano.com/posts/mysql-install-buntu/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/mysql-install-buntu/</guid>
      <description>Why  I am working on an autoposting tool for social sites. in order to complete that i want a db to log the metadata of my posts  Parts of this series  part 1 part 2  MYSQL Resources  APT install guide MYSQL config guide Post Install configuration  Install Download the config files go to this link and download the script.
https://dev.mysql.com/downloads/repo/apt/
Install the release package with dpkg  note that the w.</description>
    </item>
    
    <item>
      <title>Sync Gh Submodules Across a Super Project</title>
      <link>jnapolitano.com/posts/gh_submodule_sync/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gh_submodule_sync/</guid>
      <description>Overview This script is designed to initialize and update all submodules in a GitHub repository to the latest commits from their respective remote repositories. It ensures that all submodules, including nested submodules, are synchronized with their remote counterparts.
Prerequisites  Ensure that you have Git installed on your system. Ensure that you have cloned the repository containing the submodules.  Usage  Save the script to a file, for example, sync_submodules.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run Job Scraper</title>
      <link>jnapolitano.com/posts/l_o_c_scraper/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/l_o_c_scraper/</guid>
      <description>Library of Congress Scraper Job This repo scrapes the library of congress for all of the US Supreme Court Cases available on their platform. I intent to use this data to create a research tool to better understand the corpus of text.
Quick History of this project I had started work on this as an undergraduate at university, but the chatbot apis were not yet available.. and training modesl were far too expensive.</description>
    </item>
    
  </channel>
</rss>
