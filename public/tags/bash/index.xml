<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bash on Justin Napolitano</title>
    <link>jnapolitano.com/tags/bash/</link>
    <description>Recent content in bash on Justin Napolitano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>jayburdindustries</copyright>
    <lastBuildDate>Thu, 11 Jul 2024 16:26:32 -0500</lastBuildDate><atom:link href="jnapolitano.com/tags/bash/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Create and Deploy Cloud Run Job Script</title>
      <link>jnapolitano.com/posts/create_deploy_cloud_run_job/</link>
      <pubDate>Thu, 11 Jul 2024 16:26:32 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/create_deploy_cloud_run_job/</guid>
      <description>Cloud Run Job Deployment Script This repository contains a script to build and deploy a Python application as a Cloud Run Job using Google Cloud Build. The script dynamically generates a cloudbuild.yaml file and submits it to Google Cloud Build.
Prerequisites Before using the deployment script, ensure you have the following:
 Google Cloud SDK: Installed and configured. Docker: Installed. Google Cloud Project: Created and configured. Service Account Key: A service account key JSON file with appropriate permissions stored at keys/service-account-key.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run: LOC Flattener</title>
      <link>jnapolitano.com/posts/loc_normalizer/</link>
      <pubDate>Thu, 11 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/loc_normalizer/</guid>
      <description>Library of Congress Normalizer Job This repo normalizes the existing library of congress schema into a db that wil then be used to construct a knowledge graph of supreme court law.
Plan  Setup a venv to run locally Install requirements Write out the script to interface with gcp Set up a docker container and test locally build the image upload to gcp create the job  Setup the venv Install I installed virtualenv locally on ubuntu</description>
    </item>
    
    <item>
      <title>Install MySQL Server on Ubuntu</title>
      <link>jnapolitano.com/posts/mysql-install-buntu/</link>
      <pubDate>Fri, 28 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/mysql-install-buntu/</guid>
      <description>Why  I am working on an autoposting tool for social sites. in order to complete that i want a db to log the metadata of my posts  Parts of this series  part 1 part 2  MYSQL Resources  APT install guide MYSQL config guide Post Install configuration  Install Download the config files go to this link and download the script.
https://dev.mysql.com/downloads/repo/apt/
Install the release package with dpkg  note that the w.</description>
    </item>
    
    <item>
      <title>Sync Gh Submodules Across a Super Project</title>
      <link>jnapolitano.com/posts/gh_submodule_sync/</link>
      <pubDate>Thu, 27 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gh_submodule_sync/</guid>
      <description>Overview This script is designed to initialize and update all submodules in a GitHub repository to the latest commits from their respective remote repositories. It ensures that all submodules, including nested submodules, are synchronized with their remote counterparts.
Prerequisites  Ensure that you have Git installed on your system. Ensure that you have cloned the repository containing the submodules.  Usage  Save the script to a file, for example, sync_submodules.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run Job Scraper</title>
      <link>jnapolitano.com/posts/l_o_c_scraper/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/l_o_c_scraper/</guid>
      <description>Library of Congress Scraper Job This repo scrapes the library of congress for all of the US Supreme Court Cases available on their platform. I intent to use this data to create a research tool to better understand the corpus of text.
Quick History of this project I had started work on this as an undergraduate at university, but the chatbot apis were not yet available.. and training modesl were far too expensive.</description>
    </item>
    
  </channel>
</rss>
