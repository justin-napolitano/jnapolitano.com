<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>python on Justin Napolitano</title>
    <link>jnapolitano.com/tags/python/</link>
    <description>Recent content in python on Justin Napolitano</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>jayburdindustries</copyright>
    <lastBuildDate>Wed, 19 Jun 2024 13:36:12 -0500</lastBuildDate><atom:link href="jnapolitano.com/tags/python/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Configure Hugo XML Output for RSS Feed</title>
      <link>jnapolitano.com/posts/hugo-rss-setup/</link>
      <pubDate>Wed, 19 Jun 2024 13:36:12 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/hugo-rss-setup/</guid>
      <description>Why I have a mysql db that will be used to store values read from the rss feed of my hugo site. I need some add some keys to help with organization
Parts of this series  part 1 part 2 part 3 part 4  Resources  Hugo Page Resources Hugo Page Params Hugo RSS Templates  RSS Config Copy over the posts/rss.xml file from your theme From hugo root you would do something like&amp;hellip;</description>
    </item>
    
    <item>
      <title>Configure mysql server on ubuntu</title>
      <link>jnapolitano.com/posts/mysql-config/</link>
      <pubDate>Wed, 19 Jun 2024 13:20:56 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/mysql-config/</guid>
      <description>Why I installed mysql in the previous post. Now I need to setup users, create a db, and create a table.
Parts of this series  part 1 part 2 part 3  MYSQL Resources  APT install guide MYSQL config guide  Create a new user Login as Root mysql -u root -p Create some users In my case I will create 4 users accounts.
 Cobra@localhost Cobra@jnapolitano.com admin@localhost dummy@localhost  dummy is just used to test service connection and has not access grants or writes</description>
    </item>
    
    <item>
      <title>Automate Posting Hugo Blog to Social Sites...Second Attempt</title>
      <link>jnapolitano.com/posts/python-rss-reader/</link>
      <pubDate>Sat, 15 Jun 2024 22:36:34 -0500</pubDate>
      
      <guid>jnapolitano.com/posts/python-rss-reader/</guid>
      <description>Thoughts on This Second Pass  I will create a script that parses the sites rss feed&amp;hellip; it will then traverse the xml tree entries&amp;hellip; if a date is newer than the last publish date&amp;hellip; publish that post&amp;hellip;  I am still thinking through how to publish. I will likely write a monolithic script here, but ideally I would write an api or a batch processor to handle this in some way.</description>
    </item>
    
    <item>
      <title>Automate Posting Hugo Blog to Social Sites... Failure</title>
      <link>jnapolitano.com/posts/hugo-social-publisher/</link>
      <pubDate>Sat, 15 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/hugo-social-publisher/</guid>
      <description>Why I have a hugo blog that is a pian to share across my social feeds. I want to automate it.
Create a mockup For this I quickly sketched out my thoughts onto a writing pad. My thinking is that I will drop a yaml file into each post directory to be read by a a python application calling social apis.
Mockup As you can see this is very rudimentary. I will traverse the posts directories looking for a publish.</description>
    </item>
    
    <item>
      <title>Hugo Build and Deploy GH Workflow</title>
      <link>jnapolitano.com/posts/gh-pages-workflow/</link>
      <pubDate>Tue, 11 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gh-pages-workflow/</guid>
      <description>Creating a GH Workflow to Build and Deploy a hugo site to gh-pages Why To simplify the build process.
Creating the Workflow create your yaml config file touch hugo.yaml Set the trigger and the environment defaults The code below creates a trigger on push from the main and the gh-pages branches. It also sets read and write permissions to permit executing code and building hugo.
on:# Runs on pushes targeting the default branchpush:branches:- main# - pit# - ghpages- gh-pages# Allows you to run this workflow manually from the Actions tabworkflow_dispatch:# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pagespermissions:contents:readpages:writeid-token:write# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.</description>
    </item>
    
    <item>
      <title>GCP Cloud Run Job Scraper</title>
      <link>jnapolitano.com/posts/loc_scraper/</link>
      <pubDate>Sun, 28 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/loc_scraper/</guid>
      <description>Library of Congress Scraper Job This repo scrapes the library of congress for all of the US Supreme Court Cases available on their platform. I intent to use this data to create a research tool to better understand the corpus of text.
Quick History of this project I had started work on this as an undergraduate at university, but the chatbot apis were not yet available.. and training modesl were far too expensive.</description>
    </item>
    
    <item>
      <title>Creating a GCP Client Tool</title>
      <link>jnapolitano.com/posts/gcputils/</link>
      <pubDate>Sat, 27 Apr 2024 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/gcputils/</guid>
      <description>Creating a GCP Client Tool in Python Why I often find myself reuuing the same bits of code when working with GCP. It is very important to avoid creating multiple development trees of the same classes. I have done this before for large projects. It will lead to a very difficult to maintain stack of tools that will spaghettify over time.
Creating the submodule Create an empty directory mkdir gcpuptils Add the following code to a gcpclient.</description>
    </item>
    
    <item>
      <title>Annual Cost of Living Monte Carlo Models</title>
      <link>jnapolitano.com/posts/cost-of-living-projections/</link>
      <pubDate>Wed, 01 Jun 2022 15:24:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/cost-of-living-projections/</guid>
      <description>Cost of Living Projections Introduction I do not like negotiating for salary. Especially, without valid projections to determine a range.
I prepared this report to estimate a salary expectation that will maintain my current standard of living.
I present two Monte Carlo models of Houston and NYC annual living costs. The data is somewhat dated and &amp;ndash;particularly in the case of houston&amp;ndash; are high level estimates.
In order to produce a better report, I am currently scraping data from the internet for more accurate sample distributions.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI Part 8: Case Nodes Sample Data</title>
      <link>jnapolitano.com/posts/legal-research-part-8/</link>
      <pubDate>Mon, 23 May 2022 16:30:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-8/</guid>
      <description>Introduction The legal Research with AI Series is expanding quickly. This is the 9th post related to it in someway. Building this pipeline and integrating multiple datasets into nodes is proving to be verbose.
This post documents merging Oyez and Library of Congress of data structured json files that represent nodes and hierarchal relationships.
Main Function The plan for this program is to:
 Read the prepared dataframe created in the legal research part 7 post for each row of the df load a dictionary from the the libary of congress path and the oyez path Set keys on the Oyez dataset Write the updated Oyez dataset to file  Review the work below:</description>
    </item>
    
    <item>
      <title>Legal Research with AI: Part 6</title>
      <link>jnapolitano.com/posts/masterpiece/</link>
      <pubDate>Sat, 21 May 2022 19:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/masterpiece/</guid>
      <description>Introduction Currently, my server is mining data from the neo4j graph database.
This post contains a control model completed in SPSS that will serve as the base line for comparison to Ml models. It includes a short literature review of the theoritical foundation of the Attitudinal Model of Judicial Behavior.
Hypothesis When considering the individual Supreme Court Justices, their voting behavior may be best described by the interaction between their ideological precepts and the facts of the case.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 5</title>
      <link>jnapolitano.com/posts/legal-research-part-5/</link>
      <pubDate>Sat, 21 May 2022 14:30:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-5/</guid>
      <description>Legal Research with AI: Part 5&amp;quot; In the previous posts in this series, I have downloaded the data required to build the neo4j graph. In this post, I will arrange the data into a data structure that will permit me to easily create graph nodes and most importantly relationships.
The Runner Program The raw structure of the data is organized by the results of the api requests. There are thus 80 cases per file.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 4</title>
      <link>jnapolitano.com/posts/legal-research-part-4/</link>
      <pubDate>Thu, 19 May 2022 22:30:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-4/</guid>
      <description>Conduct Legal Research with AI Part 4 This is the fourth post in a series documenting the process of building an ml pipeline used to train models to predict the outcomes of Supreme Court cases.
You can find the others at:
 Part 1: blog.jnapolitano.io/neo4j_integration/ Part 2: blog.jnapolitano.io/constitution_to_neo/ Part 3: blog.jnapolitano.io/ai-proof-of-concept/  Modeling the Supreme Court Thankfully, much of the ground work has been done by contributors to The Washington University of St.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 3</title>
      <link>jnapolitano.com/posts/legal-research-part-3/</link>
      <pubDate>Wed, 18 May 2022 14:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-3/</guid>
      <description>Conduct Legal Research with AI: Part 3 This is the third post in a series documenting the process of building an ml pipeline that will be used to train models to predict the outcomes of Supreme Court Cases.
You can find the others at:
 blog.jnapolitano.io/neo4j_integration/ blog.jnapolitano.io/constitution_to_neo/  Introduction In this post, I will be testing a sample TensorFlow pipeline against the Supreme Court Database maintained by the Washington University Law School in order to build a proof of concept model for a Supreme Court Graph Analysis project.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 2</title>
      <link>jnapolitano.com/posts/legal-reserch-part-2/</link>
      <pubDate>Tue, 17 May 2022 18:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-reserch-part-2/</guid>
      <description>Integrating the Constitution to Neo4j I am currenlty building a graph database of Supreme Court cases in neo4j to model the behavior and decison making of the court.
In this post, I include the classes that I will be using to create individual nodes for the articles, sections, clauses, and subclauses of the Consititution.
Later, these will be related to cases and subjecst in order to train a tensorflow algorithm to recommend case law by issue area and to predict the outcome of cases.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 1</title>
      <link>jnapolitano.com/posts/legal-research-part-1/</link>
      <pubDate>Mon, 16 May 2022 14:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-1/</guid>
      <description>Introduction In a previous post, I detailed the process of crawling the Library of Congress API to generate json files that could be intergrated into you DB of choice.
In this discussion, we will integrate JSON data into a Neo4j graph database.
Overview The process is fairly straightforward. The most difficult part is wrangling your json data into the right format for integration.
The main function first instantiates the database config informormation.</description>
    </item>
    
    <item>
      <title>Conduct Legal Research with AI: Part 0</title>
      <link>jnapolitano.com/posts/legal-research-part-0/</link>
      <pubDate>Mon, 16 May 2022 13:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/legal-research-part-0/</guid>
      <description>Crawling the Library of Congress API Introduction The United States Library of Congress maintains a rest api for developers to crawl their collections. It is an open source tool that anyone can access in order to conduct research. Check out the documenation at https://libraryofcongress.github.io/data-exploration/.
Creating a crawler I took the approach of writing a generator that produces a search result page object that can be operated upon with each iteration.</description>
    </item>
    
    <item>
      <title>Feasibility of Transatlantic Carbon Shipping</title>
      <link>jnapolitano.com/posts/feasibility_study_shipping_carbon/</link>
      <pubDate>Sat, 07 May 2022 18:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/feasibility_study_shipping_carbon/</guid>
      <description>Feasibility of Shipping Carbon Across the Atlantic Methodology Please review the my previous post which details the design of my model.
The major difference in this report is the sampling of the mean price per voyage.
The conversion error has been corrected.
Findings Distance of Transport The standard deviation in mean price per voyage was found to be less than 0. This suggests that the distance of travel within Europe is marginal.</description>
    </item>
    
    <item>
      <title>Monte Carlo Projection of the Annual Cost of Shipping Carbon from Europe to the United States</title>
      <link>jnapolitano.com/posts/carbon-shipping-projections/</link>
      <pubDate>Sat, 07 May 2022 18:40:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/carbon-shipping-projections/</guid>
      <description>A Monte Carlo Projection of the Annual Cost of Shipping Carbon from Europe to US Ports Revision An earlier version of this report had reported values that were erroneously elevated by a decimal error in the meters to km conversion of distances between ports. I apologize for unintentionally publishing misleading information.
Situations like this one, are why I always publish with code. Transparency is the key to rigor.
Introduction I produced an earlier report that identified potential ports that may be suitable terminals for carbon imports into the United States for storage or industrial applications.</description>
    </item>
    
    <item>
      <title>Potential Carbon and Hydrogen Storage Facilities Near Import/Export Ports</title>
      <link>jnapolitano.com/posts/wells-near-ports/</link>
      <pubDate>Fri, 06 May 2022 08:30:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/wells-near-ports/</guid>
      <description>Potential Carbon Storage Facilities Near Import/Export Ports Import and Procedural Functions import pandas as pd import matplotlib.pyplot as plt import geopandas as gpd import folium import contextily as cx import rtree from zlib import crc32 import hashlib from shapely.geometry import Point, LineString, Polygon import numpy as np from scipy.spatial import cKDTree from shapely.geometry import Point from haversine import Unit from geopy.distance import distance Restrictions  Must be near a pipeline terminal Must be Near a petrolium Port  Query Plan Imports</description>
    </item>
    
    <item>
      <title>Potential Carbon and Hydrogen Storage Wells Near Pipelines</title>
      <link>jnapolitano.com/posts/wells-near-pipelines/</link>
      <pubDate>Fri, 06 May 2022 05:30:32 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/wells-near-pipelines/</guid>
      <description>Potential Carbon Storage Wells Near Pipelines Import and Procedural Functions import pandas as pd import matplotlib.pyplot as plt import geopandas as gpd import folium import contextily as cx import rtree from zlib import crc32 import hashlib from shapely.geometry import Point, LineString, Polygon import numpy as np from scipy.spatial import cKDTree from shapely.geometry import Point from haversine import Unit from geopy.distance import distance Query Plan Restrictions  Must be near a pipeline terminal  Imports  Pipeline Data Well Data  Filtering   For each well calculate nearest pipeline</description>
    </item>
    
    <item>
      <title>EuroZone Gas Imports and Exports</title>
      <link>jnapolitano.com/posts/european-gas-imports/</link>
      <pubDate>Thu, 05 May 2022 00:00:00 +0000</pubDate>
      
      <guid>jnapolitano.com/posts/european-gas-imports/</guid>
      <description>EuroZone Gas Imports and Exports Import and Procedural Functions import pandas as pd import matplotlib.pyplot as plt import numpy as np from geopy.distance import distance import re import matplotlib.pyplot as plt from IPython.core.display import HTML #import IPython.core.display as display def hugo_safe_render(styler): &amp;#34;&amp;#34;&amp;#34; Removes spacing from HTML output of df.style to prevent rendering issues in Hugo. &amp;#34;&amp;#34;&amp;#34; raw_html = styler.to_html() clean_html = &amp;#39; &amp;#39;.join(raw_html.split()) return HTML(clean_html) def magnify(): return [dict(selector=&amp;#34;th&amp;#34;, props=[(&amp;#34;font-size&amp;#34;, &amp;#34;4pt&amp;#34;)]), dict(selector=&amp;#34;td&amp;#34;, props=[(&amp;#39;padding&amp;#39;, &amp;#34;0em 0em&amp;#34;)]), dict(selector=&amp;#34;th:hover&amp;#34;, props=[(&amp;#34;font-size&amp;#34;, &amp;#34;12pt&amp;#34;)]), dict(selector=&amp;#34;tr:hover td:hover&amp;#34;, props=[(&amp;#39;max-width&amp;#39;, &amp;#39;200px&amp;#39;), (&amp;#39;font-size&amp;#39;, &amp;#39;12pt&amp;#39;)]) ] Query Strategy Imports TO many to list&amp;hellip; I&amp;rsquo;ll ad them as I go below</description>
    </item>
    
  </channel>
</rss>
